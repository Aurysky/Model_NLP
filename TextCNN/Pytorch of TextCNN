import numpy as np
import torch.nn as nn
import torch
import torch.optim as optim

sentences  = ['i love you','he love me','she like basketball','i hate you','sorry for that','this is awful']
lables = [1,1,1,0,0,0]
word_list = " ".join(sentences).split()#语法'sep'.join(seq)其中sep为指定分隔符,seq为连接元素的序列
word_list = list(set(word_list))
word_dict = {k:v for v ,k in enumerate(word_list)}
vocab_size = len(word_dict)

inputs_ids = []
target = []
for sen in sentences:
    inputs_ids.append(np.asarray([word_dict[n] for n in sen.split()]))
for lable in lables:
    target.append(lable)
inputs = torch.LongTensor(inputs_ids)
print(inputs.shape)
targets = torch.LongTensor(target)


class TextCNN_Config(object):
    def __init__(self,num_filters,filter_size,embedding_size,vocab_size,sequence_size,num_class):
        self.num_filters = num_filters
        self.filter_size =  filter_size
        self.vocab_size = vocab_size
        self.embedding_size = embedding_size
        self.sequence_size = sequence_size
        self.num_class = num_class

class TextCNN(nn.Module):
    def __init__(self,config):
        super(TextCNN,self).__init__()
        self.num_filters = config.num_filters
        self.filter_sizes = config.filter_size
        self.embedding_size = config.embedding_size
        self.vocab_size = config.vocab_size
        self.sequence_size = config.sequence_size
        self.num_class = config.num_class
        self.embdded = nn.Embedding(self.vocab_size,self.embedding_size)

        self.activation = nn.ReLU()

        self.linear = nn.Linear(self.num_filters*len(self.filter_sizes),self.num_class)

    def forward(self,input_ids): #input_ids [batch_size,seq_size]
        embedd_output = self.embdded(input_ids)  #[batch_size,seq_size,embedding_size]
        embedd_output = embedd_output.unsqueeze(1)  #增加输入通道[batch_size,1,seq_size,emdedding_size]
        hidden_out = []
        for filter_size in self.filter_sizes:
            conv_output = self.activation(nn.Conv2d(1,self.num_filters,(filter_size,self.embedding_size))(embedd_output))
            # [batch_size,num_filters,sequence_size-filter_size+1,1]
            maxpool_out = nn.MaxPool2d((self.sequence_size - filter_size + 1, 1))(conv_output)

            hidden_out.append(maxpool_out)
        output = torch.cat(hidden_out,dim=1)
        output = torch.reshape(output,[-1,len(self.filter_sizes)*self.num_filters])
        logit = self.linear(output)
        return logit

config = TextCNN_Config(num_filters=2,filter_size=[2,2,2],embedding_size=3,vocab_size=len(word_dict),sequence_size=3,num_class=2)
model = TextCNN(config)



criterion = nn.CrossEntropyLoss()

optimizer = optim.Adam(model.parameters(), lr=0.001)
#
#
#
# # Training
#
for epoch in range(5000):
#
     optimizer.zero_grad()
#
     output = model(inputs)
#
#
#
#     # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)
#
     loss = criterion(output, targets)
#
     if (epoch + 1) % 1000 == 0:
#
         print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))
#
#
#
     loss.backward()
#
     optimizer.step()
#
#
#
# # Test
#
test_text = 'sorry hate you'
#
tests = [np.asarray([word_dict[n] for n in test_text.split()])]
#
test_batch = (torch.LongTensor(tests))
#
#
#
# # Predict
#
predict = model(test_batch).data.max(1, keepdim=True)[1]
#
if predict[0][0] == 0:
#
     print(test_text,"is Bad Mean...")
#
else:
#
     print(test_text,"is Good Mean!!")


