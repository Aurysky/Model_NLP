from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import copy
import math
import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
import torch.nn.functional as F
import numpy as np


def gelu(x):
    """Implementation of the gelu activation function.
        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):
        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
    """
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))

#Bert的配置信息
class BertConfig(object):
    """Configuration class to store the configuration of a `BertModel`.
    """
    def __init__(self,
                vocab_size=98,
                hidden_size=784,
                max_length = 1,
                num_hidden_layers=3,
                num_attention_heads=2,
                intermediate_size=3136,
                hidden_act="gelu",
                hidden_dropout_prob=0.1,
                attention_probs_dropout_prob=0.1,
                num_labels=10):
        '''Args:
                vocab_size:输入词汇表的维度
                hidden_size:隐层维度，
                num_hidden_layers:bert模型中层数
                num_attention_heads:多头注意力头的个数
                intermediate_size:前馈神经网络维度
                hidden_act:隐层激活函数
                hidden_dropout_prob:隐层的dropout
                attention_probs_dropout_prob:注意力里的dropout
        '''
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.max_length = max_length
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.hidden_act = hidden_act
        self.intermediate_size = intermediate_size
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.num_labels = num_labels
#可选的部分配置信息
class FastBertConfig(object):
    def __init__(self,hidden_size=784,num_attention_heads=2,num_labels=10,attention_probs_dropout_prob=0.1):
        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.num_labels = num_labels
        self.attention_probs_dropout_prob = attention_probs_dropout_prob


#调试实例
bert_config = BertConfig()
fast_bert_config =FastBertConfig()



class BERTLayerNorm(nn.Module):
    def __init__(self,bert_config,eps=1e-12):
        super(BERTLayerNorm,self).__init__()
        self.gamma = nn.Parameter(torch.ones(bert_config.hidden_size))
        self.beta = nn.Parameter(torch.zeros(bert_config.hidden_size))
        self.eps = eps

    def forward(self,x):
        u = x.mean(-1,keepdim=True)
        s = (x - u).pow(2).mean(-1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        return (self.gamma).cuda()* x + (self.beta).cuda()




class BERTEmbeddings(nn.Module):
    def __init__(self,bert_config):
        super(BERTEmbeddings,self).__init__()
        self.word_embedding = nn.Embedding(bert_config.vocab_size,bert_config.hidden_size)
        self.position_embedding = nn.Embedding(bert_config.max_length,bert_config.hidden_size)
        self.LayerNorm = BERTLayerNorm(bert_config)

    def forward(self,x):
        max_length = x.size(1)  #第一维度为batch,后面第二维度为序列的长度
        position_ids = torch.arange(max_length,dtype=torch.long)
        position_ids = position_ids.unsqueeze(0).expand_as(x)
        word_embeddings = self.word_embedding(x)
        position_embeddings = self.position_embedding(position_ids)
        embeddings = word_embeddings  + position_embeddings
        return self.LayerNorm(embeddings)


class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()

    def forward(self,q,k,v):
        #第一步计算分数
        score = torch.matmul(q,k.transpose(-1,-2))/np.sqrt(8)#第三维度是其向量的维度
        #print(score.shape)
        atten = nn.Softmax(dim=-1)(score)
        context = torch.matmul(atten,v)
        return context,atten


class MultiHeadAttention(nn.Module):
    def __init__(self,bert_config):
        super(MultiHeadAttention, self).__init__()
        self.bert_config = bert_config
        self.bert_config.hidden_size = bert_config.hidden_size
        self.bert_config.num_attention_heads = bert_config.num_attention_heads
        self.dim_per_head = bert_config.hidden_size//bert_config.num_attention_heads  #python3里面用//才能得到Int数据
        self.W_Q = nn.Linear(bert_config.hidden_size, self.dim_per_head*bert_config.num_attention_heads)
        self.W_K = nn.Linear(bert_config.hidden_size, self.dim_per_head*bert_config.num_attention_heads)
        self.W_V = nn.Linear(bert_config.hidden_size, self.dim_per_head*bert_config.num_attention_heads)
        self.linear = nn.Linear(bert_config.num_attention_heads*self.dim_per_head,self.bert_config.hidden_size)
        self.dropout = nn.Dropout(bert_config.attention_probs_dropout_prob)

    def forward(self, Q, K, V):
        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]
        batch_size = Q.size(0)#作为残差连接
        Q = BERTLayerNorm(self.bert_config)(Q)
        K = BERTLayerNorm(self.bert_config)(K)
        V = BERTLayerNorm(self.bert_config)(V)
        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)
        q_s = self.W_Q(Q).view(batch_size, -1, self.bert_config.num_attention_heads, self.dim_per_head).transpose(1,2)  # q_s: [batch_size x num_heads x len_q x dim_per_head]
        k_s = self.W_K(K).view(batch_size, -1, self.bert_config.num_attention_heads, self.dim_per_head).transpose(1,2)  # k_s: [batch_size x num_heads x len_k x dim_per_head]
        v_s = self.W_V(V).view(batch_size, -1, self.bert_config.num_attention_heads, self.dim_per_head).transpose(1,2)  # v_s: [batch_size x num_heads x len_k x dim_per_head]

        #attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]

        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]
        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.bert_config.num_attention_heads * self.dim_per_head)#context: [batch_size x len_q x n_heads * d_v]
        output = self.linear(context)
        return self.dropout(output) , attn # output: [batch_size x len_q x d_model]

class PoswiseFeedForwardNet(nn.Module):
    def __init__(self,bert_config):
        super(PoswiseFeedForwardNet, self).__init__()
        self.fc1 = nn.Linear(bert_config.hidden_size, bert_config.intermediate_size)
        self.fc2 = nn.Linear(bert_config.intermediate_size, bert_config.hidden_size)
        self.dropout = nn.Dropout(bert_config.hidden_dropout_prob)

    def forward(self, x):
        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)
        return self.dropout(self.fc2(gelu(self.fc1(x))))




class BERTLayer(nn.Module):
    def __init__(self,bert_config):
        super(BERTLayer,self).__init__()
        self.residual_weight = nn.Parameter(torch.Tensor([0]))
        self.attn = MultiHeadAttention(bert_config)
        self.pos_forward = PoswiseFeedForwardNet(bert_config)

    def forward(self,input):
        residual = input
        outputs,attn = self.attn(input,input,input)
        outputs = residual + self.residual_weight*outputs
        residual_output = outputs
        outputs = self.pos_forward(outputs)
        return residual_output+self.residual_weight*outputs
#调试

# encoder_layer = BERTLayer(bert_config)
# src = torch.rand(10, 1, 784)
# encoder_out = encoder_layer(src)
# print(encoder_out.shape)

class BERTPool(nn.Module):
    def __init__(self,bert_config):
        super(BERTPool,self).__init__()
        self.linear_pool = nn.Linear(bert_config.hidden_size,bert_config.hidden_size)
        self.activation = nn.ReLU()

    def forward(self,hidden_state):
        pool_state = hidden_state[:,0]
        pooled_output = self.linear_pool(pool_state)
        pooled_output = self.activation(pooled_output)
        return pooled_output
#调试
# encoder_pool = BERTPool(bert_config)
# pool_output = encoder_pool(encoder_out)
# print(pool_output.shape)
# print(pool_output)

class CommonClassifier(nn.Module):#处理经pool以后的分类
    def __init__(self,bert_config):
        super(CommonClassifier,self).__init__()
        self.classifier = nn.Linear(bert_config.hidden_size,bert_config.num_labels)
        self.dropout = nn.Dropout(bert_config.hidden_dropout_prob)


    def forward(self,pooled_output):
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

#classifier = CommonClassifier(hidden_size=512,num_labels=10,dropout=0.2)
#logits = classifier(pool_output)
#print(logits)
#print(logits.shape)


class FastBERTClassifier(nn.Module):
    def __init__(self,bert_config,fast_bert_config):
        super(FastBERTClassifier,self).__init__()
        self.dense_narrow = nn.Linear(bert_config.hidden_size,fast_bert_config.hidden_size)
        self.attn = MultiHeadAttention(fast_bert_config)
        self.dense_prelogit = nn.Linear(fast_bert_config.hidden_size,fast_bert_config.hidden_size)
        self.dense_logits = nn.Linear(fast_bert_config.hidden_size,fast_bert_config.num_labels)

    def forward(self,hidden_state):
        output = self.dense_narrow(hidden_state)
        attn_output ,attn = self.attn(output,output,output)
        pre_logits = self.dense_prelogit(attn_output[:,0])
        logits = self.dense_logits(pre_logits)
        #prob = F.softmax(logits,dim=-1) 计算logit概率，维度为最后维度，也就是输出词汇的维度。
        return logits #

#fastbertclassifier = FastBERTClassifier(hidden_size=512,hidden_size_fast=128,num_heads_fast=4,num_class=10)
#fast_logits = fastbertclassifier(pool_output)
#print(fast_logits)
#print(fast_logits.shape)
#print(prob)


class FastBERTGraph(nn.Module):
    def __init__(self,bert_config,fast_bert_config):
        super(FastBERTGraph,self).__init__()
        self.num_classes = torch.tensor(bert_config.num_labels,dtype=torch.float32)
        bert_layer = BERTLayer(bert_config)
        self.layers = nn.ModuleList([copy.deepcopy(bert_layer) for _ in range(bert_config.num_hidden_layers)])


        self.layer_classifier = FastBERTClassifier(bert_config,fast_bert_config)

        self.layer_classifiers = nn.ModuleDict()

        for i in range(bert_config.num_hidden_layers-1):
            self.layer_classifiers['branch_classifer_'+str(i)] = copy.deepcopy(self.layer_classifier)
        self.layer_classifiers['final_classifier'] = copy.deepcopy(self.layer_classifier)
        self.ce_loss_fct = CrossEntropyLoss()

    def forward(self,hidden_states,labels=None,inference=False,inference_speed=0.5,training_stage=0):
        if inference:
            uncertain_infos = []
            for i ,(layer_module,(k,layer_classifier_module)) in enumerate(zip(self.layers,self.layer_classifiers.items())):
                hidden_states = layer_module(hidden_states) #接受embedding以后输出作为每一层transformer的输入
                logits = layer_classifier_module(hidden_states)
                prob = F.softmax(logits,dim=-1)
                log_prop = F.log_softmax(logits,dim=-1)
                uncertain = torch.sum(prob*log_prop,1)/(-torch.log(self.num_classes))
                uncertain_infos.append(uncertain)
                if uncertain < inference_speed:
                    return prob,i,uncertain_infos
            return prob,i,uncertain_infos
        else:
            if training_stage == 0: #训练阶段
                for layer_module in self.layers:
                    hidden_states = layer_module(hidden_states)
                pool_output = self.pool(hidden_states)
                logits = self.layer_classifier(pool_output)  #最后一层的隐藏状态作为输入
                loss =  self.ce_loss_fct(logits,labels)
                return loss,logits
            else:#蒸馏阶段
                all_encoder_layers = []
                for layer_module in self.layers:
                    hidden_states = layer_module(hidden_states)
                    all_encoder_layers.append(hidden_states)
                all_logits = []
                for encoder_layer,(k,layer_module_classifier) in zip(all_encoder_layers,self.layer_classifiers.items()):
                    layer_logits = layer_module_classifier(encoder_layer)
                    all_logits.append(layer_logits)
                loss =0.0
                teacher_log_prob = F.log_softmax(all_logits[-1],dim=-1)
                for student_logits in all_logits[:-1]:
                    student_prob = F.softmax(student_logits,dim=-1)
                    student_log_prob = F.log_softmax(student_logits,dim=-1)
                    uncertain = torch.sum(student_prob*student_log_prob,1)/(-torch.log(self.num_classes))
                    #print('uncertain:',uncertain[0])
                    DK_l = torch.sum(student_prob*(student_log_prob - teacher_log_prob), 1)
                    DK_l = torch.mean(DK_l)
                    loss += DK_l
                return loss ,all_logits




class FastBertModel(nn.Module):
    def __init__(self, bert_config,fast_bert_config):
        super(FastBertModel, self).__init__()
        #self.embeddings = BERTEmbeddings(bert_config)
        self.graph = FastBERTGraph(bert_config,fast_bert_config)

    def forward(self, input_ids,inference,inference_speed,labels=None,training_stage=0):

        #embedding_output = self.embeddings(input_ids)

        # if Inference=True:res=prob, else: res=loss
        res = self.graph(input_ids,inference=inference, inference_speed=inference_speed,
                            labels=labels, training_stage=training_stage,
                         )
        return res

